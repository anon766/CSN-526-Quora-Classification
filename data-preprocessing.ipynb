{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import string\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from tqdm import tqdm\n",
    "from utils import timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\tqdm\\std.py:697: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "puncts=['☹', 'Ź', 'Ż', 'ἰ', 'ή', 'Š', '＞', 'ξ','ฉ', 'ั', 'น', 'จ', 'ะ', 'ท', 'ำ', 'ใ', 'ห', '้', 'ด', 'ี', \n",
    "'่', 'ส', 'ุ', 'Π', 'प', 'ऊ', 'Ö', 'خ', 'ب', 'ஜ', 'ோ', 'ட', '', '「', 'ẽ', '½', '△', 'É', 'ķ', 'ï', '¿', \n",
    "'ł', '북', '한', '¼', '∆', '≥', '⇒', '¬', '∨', 'č', 'š', '∫', 'ḥ', 'ā', 'ī', 'Ñ', 'à', '▾', 'Ω', '＾', 'ý', \n",
    "'µ', '?', '!', '.', ',', '\"', '#', '$', '%', '\\\\', \"'\", '(', ')', '*', '+', '-', '/', ':', ';', '<', '=', \n",
    "'>', '@', '[', ']', '^', '_', '`', '{', '|', '}', '~', '“', '”', '’', 'é', 'á', '′', '…', 'ɾ', '̃', \n",
    "'ɖ', 'ö', '–', '‘', 'ऋ', 'ॠ', 'ऌ', 'ॡ', 'ò', 'è', 'ù', 'â', 'ğ', 'म', 'ि', 'ल', 'ग', 'ई', 'क', 'े', 'ज', \n",
    "'ो', 'ठ', 'ं', 'ड', 'Ž', 'ž', 'ó', '®', 'ê', 'ạ', 'ệ', '°', 'ص', 'و', 'ر', 'ü', '²', '₹', 'ú', '√', 'α', \n",
    "'→', 'ū', '—', '£', 'ä', '️', 'ø', '´', '×', 'í', 'ō', 'π', '÷', 'ʿ', '€', 'ñ', 'ç', 'へ', 'の', 'と', 'も',\n",
    "'↑', '∞', 'ʻ', '℅''ι', '•', 'ì', '−', 'л', 'я', 'д', 'ل', 'ك', 'م', 'ق', 'ا', '∈', '∩', '⊆', 'ã', 'अ', 'न',\n",
    "'ु', 'स', '्', 'व', 'ा', 'र', 'त', '§', '℃', 'θ', '±', '≤', 'उ', 'द', 'य', 'ब', 'ट', '͡', '͜', 'ʖ', '⁴', \n",
    "'™', 'ć', 'ô', 'с', 'п', 'и', 'б', 'о', 'г', '≠', '∂', 'आ', 'ह', 'भ', 'ी', '³', 'च', '...', '⌚', '⟨', '⟩',\n",
    "'∖', '˂', 'ⁿ', '⅔', 'న', 'ీ', 'క', 'ె', 'ం', 'ద', 'ు', 'ా', 'గ', 'ర', 'ి', 'చ', 'র', 'ড়', 'ঢ়', 'સ',\n",
    "'ં', 'ઘ', 'ર', 'ા', 'જ', '્', 'ય', 'ε', 'ν', 'τ', 'σ', 'ş', 'ś', 'س', 'ت', 'ط', 'ي', 'ع', 'ة', 'د', 'Å',\n",
    "'☺', 'ℇ', '❤', '♨', '✌', 'ﬁ', 'て', '„', 'Ā', 'ត', 'ើ', 'ប', 'ង', '្', 'អ', 'ូ', 'ន', 'ម', 'ា', 'ធ', 'យ',\n",
    "'វ', 'ី', 'ខ', 'ល', 'ះ', 'ដ', 'រ', 'ក', 'ឃ', 'ញ', 'ឯ', 'ស', 'ំ', 'ព', 'ិ', 'ៃ', 'ទ', 'គ', '¢', 'つ', 'や', \n",
    "'ค', 'ณ', 'ก', 'ล', 'ง', 'อ', 'ไ', 'ร', 'į', 'ی', 'ю', 'ʌ', 'ʊ', 'י', 'ה', 'ו', 'ד', 'ת', 'ᠠ', 'ᡳ', 'ᠰ', \n",
    "'ᠨ', 'ᡤ', 'ᡠ', 'ᡵ', 'ṭ', 'ế', 'ध', 'ड़', 'ß', '¸', 'ч',  'ễ', 'ộ', 'फ', 'μ', '⧼', '⧽', 'ম', 'হ', 'া', 'ব', \n",
    "'ি', 'শ', '্', 'প', 'ত', 'ন', 'য়', 'স', 'চ', 'ছ', 'ে', 'ষ', 'য', '়', 'ট', 'উ', 'থ', 'ক', 'ῥ', 'ζ', 'ὤ', 'Ü', \n",
    "'Δ', '내', '제', 'ʃ', 'ɸ', 'ợ', 'ĺ', 'º', 'ष', '♭', '़', '✅', '✓', 'ě', '∘', '¨', '″', 'İ', '⃗', '̂', 'æ', 'ɔ', \n",
    "'∑', '¾', 'Я', 'х', 'О', 'з', 'ف', 'ن', 'ḵ', 'Č', 'П', 'ь', 'В', 'Φ', 'ỵ', 'ɦ', 'ʏ', 'ɨ', 'ɛ', 'ʀ', 'ċ', 'օ', \n",
    "'ʍ', 'ռ', 'ք', 'ʋ', '兰', 'ϵ', 'δ', 'Ľ', 'ɒ', 'î', 'Ἀ', 'χ', 'ῆ', 'ύ', 'ኤ', 'ል', 'ሮ', 'ኢ', 'የ', 'ኝ', 'ን', \n",
    "'አ', 'ሁ', '≅', 'ϕ', '‑', 'ả', '￼', 'ֿ', 'か', 'く', 'れ', 'ő', '－', 'ș', 'ן', 'Γ', '∪', 'φ', 'ψ', '⊨', 'β', '∠', \n",
    "'Ó', '«', '»', 'Í', 'க', 'வ', 'ா', 'ம', '≈', '⁰', '⁷', 'ấ', 'ũ', '눈', '치', 'ụ', 'å', '،', '＝', '（', '）', \n",
    "'ə', 'ਨ', 'ਾ', 'ਮ', 'ੁ', '︠', '︡', 'ɑ', 'ː', 'λ', '∧', '∀', 'Ō', 'ㅜ', 'Ο', 'ς', 'ο', 'η', 'Σ', 'ण', '大','能', \n",
    "'化', '生', '水', '谷', '精', '微', 'ル', 'ー', 'ジ', 'ュ', '支', '那', '¹', 'マ', 'リ', '仲', '直', 'り', 'し', 'た', \n",
    "'主', '席', '血', '⅓', '漢', '髪', '金', '茶', '訓', '読', '黒', 'ř', 'あ', 'わ', 'る', '胡', '南', '수', '능', '广', \n",
    "'电', '总', 'ί', '서', '로', '가', '를', '행', '복', '하', '게', '기', '乡', '故', '爾', '汝', '言', '得', '理', '让', \n",
    "'骂', '野', '比', 'び', '太', '後', '宮', '甄', '嬛', '傳', '做', '莫', '你', '酱', '紫', '甲', '骨', '陳', '宗', '陈', \n",
    "'什', '么', '说', '伊', '藤', '長', 'ﷺ', '僕', 'だ', 'け', 'が', '街', '◦', '火', '团', '表',  '看', '他', '顺', '眼', \n",
    "'中', '華', '民', '國', '許', '自', '東', '儿', '臣', '惶', '恐', 'っ', '木', 'ホ', 'ج', '教', '官', '국', '고', '등', \n",
    "'학', '교', '는', '몇', '시', '간', '업', '니', '本', '語', '上', '手', 'で', 'ね', '台', '湾', '最', '美', '风', '景', \n",
    "'Î', '≡', '皎', '滢', '杨', '∛', '簡', '訊', '短', '送', '發', 'お', '早', 'う', '朝', 'ش', 'ه', '饭', '乱', '吃', \n",
    "'话', '讲', '男', '女', '授', '受', '亲', '好', '心', '没', '报', '攻', '克', '禮', '儀', '統', '已', '經', '失', '存', \n",
    "'٨', '八', '‛', '字', '：', '别', '高', '兴', '还', '几', '个', '条', '件', '呢', '觀', '《', '》', '記', '宋', '楚', \n",
    "'瑜', '孫', '瀛', '枚', '无', '挑', '剔', '聖', '部', '頭', '合', '約', 'ρ', '油', '腻', '邋', '遢', 'ٌ', 'Ä', '射', '籍', \n",
    "'贯', '老', '常', '谈', '族', '伟', '复', '平', '天', '下', '悠', '堵', '阻', '愛', '过', '会', '俄', '罗', '斯', '茹', \n",
    "'西', '亚', '싱', '관', '없', '어', '나', '이', '키', '夢', '彩', '蛋', '鰹', '節', '狐', '狸', '鳳', '凰', '露', '王', \n",
    "'晓', '菲', '恋', 'に', '落', 'ち', 'ら', 'よ', '悲', '反', '清', '復', '明', '肉', '希', '望', '沒', '公', '病', '配', \n",
    "'信', '開', '始', '日', '商', '品', '発', '売', '分', '子', '创', '意', '梦', '工', '坊', 'ک', 'پ', 'ڤ', '蘭', '花', '羡', \n",
    "'慕', '和', '嫉', '妒', '是', '样', 'ご', 'め', 'な', 'さ', 'い', 'す', 'み', 'ま', 'せ', 'ん', '音', '红', '宝', '书', \n",
    "'封', '柏', '荣', '江', '青', '鸡', '汤', '文', '粵', '拼', '寧', '可', '錯', '殺', '千', '絕', '放', '過', '」', '之', \n",
    "'勢', '请', '国', '知', '识', '产', '权', '局', '標', '點', '符', '號', '新', '年', '快', '乐', '学', '业', '进', '步', '身', \n",
    "'体', '健', '康', '们', '读', '我', '的', '翻', '译', '篇', '章', '欢', '迎', '入', '坑', '有', '毒', '黎', '氏', '玉', '英', \n",
    "'啧', '您', '这', '口', '味', '奇', '特', '也', '就', '罢', '了', '非', '要', '以', '此', '为', '依', '据', '对', '人', '家', \n",
    "'批', '判', '一', '番', '不', '地', '道', '啊', '谢', '六', '佬']\n",
    "punctString = ''.join([p for p in puncts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = \"train/train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "\treturn pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations(text):\n",
    "   #return ' '.join([t for t in text.split(' ') if t not in puncts])\n",
    "    #for t in text:\n",
    "     #   if t in puncts:\n",
    "      #      text = text.replace(t, \"\")\n",
    "    #return text\n",
    "    return text.translate(str.maketrans('','',punctString))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abadfaacd adf a\n",
      "Hello my name is Bruh\n"
     ]
    }
   ],
   "source": [
    "print(remove_punctuations('ab#//#adfaacd# adf #/a'))\n",
    "print(remove_punctuations('Hello, my name is Bruh!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_tokenizer():\n",
    "    return RegexpTokenizer('\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwordsList = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours']\n"
     ]
    }
   ],
   "source": [
    "print (stopwordsList[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    return ' '.join([word for word in text.split(' ') if word not in stopwordsList])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_stemmer():\n",
    "\treturn SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data, punctuation=True, tokenize=True, removeStopwords=True, stemming=True):\n",
    "\tif punctuation:\n",
    "\t\tprint('Running remove_punctuations...')\n",
    "\t\tdata['question_text'] = data['question_text'].progress_apply(lambda x: remove_punctuations(x))\n",
    "\t\tprint(data['question_text'].head(15))\n",
    "\n",
    "\tif tokenize:\n",
    "\t\tprint('Running tokenize...')\n",
    "\t\tdata['question_text'] = data['question_text'].progress_apply(lambda x: ' '.join(init_tokenizer().tokenize(x.lower())))\n",
    "\t\tprint(data['question_text'].head(15))\n",
    "\n",
    "\tif removeStopwords:\n",
    "\t\t\n",
    "\t\tprint('Removing stopwords...')\n",
    "\t\tdata['question_text'] = data['question_text'].progress_apply(lambda x: remove_stopwords(x))\n",
    "\t\tprint(data['question_text'].head(15))\n",
    "\n",
    "\tif stemming:\n",
    "\t\tprint('Stemming...')\n",
    "\t\tstemmer = init_stemmer()\n",
    "\t\tdata['question_text'] = data['question_text'].progress_apply(lambda x: ' '.join([stemmer.stem(w) for w in x.split(' ')]))\n",
    "\t\tprint(data['question_text'].head(15))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_data(data_path = TRAIN_PATH, output_file = \"train/train1111.csv\", punctuation = True, tokenize = True, remove_stopwords = True, stemming = True):\n",
    "    data = load_data(data_path)\n",
    "    clean_data(data, punctuation, tokenize, remove_stopwords, stemming)\n",
    "    if output_file :\n",
    "        data.to_csv(output_file, \n",
    "\t\t\t\t\t   index=False)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                          | 722/1306122 [00:00<03:18, 6580.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running remove_punctuations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 1306122/1306122 [02:05<00:00, 10366.44it/s]\n",
      "  0%|▎                                                       | 5983/1306122 [00:00<00:22, 57242.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     How did Quebec nationalists see their province...\n",
      "1     Do you have an adopted dog how would you encou...\n",
      "2     Why does velocity affect time Does velocity af...\n",
      "3     How did Otto von Guericke used the Magdeburg h...\n",
      "4     Can I convert montra helicon D to a mountain b...\n",
      "5     Is Gaza slowly becoming Auschwitz Dachau or Tr...\n",
      "6     Why does Quora automatically ban conservative ...\n",
      "7     Is it crazy if I wash or wipe my groceries off...\n",
      "8     Is there such a thing as dressing moderately a...\n",
      "9     Is it just me or have you ever been in this ph...\n",
      "10                      What can you say about feminism\n",
      "11                  How were the Calgary Flames founded\n",
      "12    What is the dumbest yet possibly true explanat...\n",
      "13    Can we use our external hard disk as a OS as w...\n",
      "14    I am 30 living at home and have no boyfriend I...\n",
      "Name: question_text, dtype: object\n",
      "Running tokenize...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 1306122/1306122 [00:15<00:00, 85875.84it/s]\n",
      "  0%|                                                          | 297/1306122 [00:00<07:22, 2948.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     how did quebec nationalists see their province...\n",
      "1     do you have an adopted dog how would you encou...\n",
      "2     why does velocity affect time does velocity af...\n",
      "3     how did otto von guericke used the magdeburg h...\n",
      "4     can i convert montra helicon d to a mountain b...\n",
      "5     is gaza slowly becoming auschwitz dachau or tr...\n",
      "6     why does quora automatically ban conservative ...\n",
      "7     is it crazy if i wash or wipe my groceries off...\n",
      "8     is there such a thing as dressing moderately a...\n",
      "9     is it just me or have you ever been in this ph...\n",
      "10                      what can you say about feminism\n",
      "11                  how were the calgary flames founded\n",
      "12    what is the dumbest yet possibly true explanat...\n",
      "13    can we use our external hard disk as a os as w...\n",
      "14    i am 30 living at home and have no boyfriend i...\n",
      "Name: question_text, dtype: object\n",
      "Stemming...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████| 1306122/1306122 [05:19<00:00, 4087.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     how did quebec nationalist see their provinc a...\n",
      "1     do you have an adopt dog how would you encoura...\n",
      "2     whi doe veloc affect time doe veloc affect spa...\n",
      "3     how did otto von guerick use the magdeburg hem...\n",
      "4     can i convert montra helicon d to a mountain b...\n",
      "5     is gaza slowli becom auschwitz dachau or trebl...\n",
      "6     whi doe quora automat ban conserv opinion when...\n",
      "7     is it crazi if i wash or wipe my groceri off g...\n",
      "8     is there such a thing as dress moder and if so...\n",
      "9     is it just me or have you ever been in this ph...\n",
      "10                         what can you say about femin\n",
      "11                     how were the calgari flame found\n",
      "12    what is the dumbest yet possibl true explan fo...\n",
      "13    can we use our extern hard disk as a os as wel...\n",
      "14    i am 30 live at home and have no boyfriend i w...\n",
      "Name: question_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "data = load_clean_data(data_path = TRAIN_PATH, output_file = \"train/train1101.csv\", punctuation = True, tokenize = True, remove_stopwords = False, stemming = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
